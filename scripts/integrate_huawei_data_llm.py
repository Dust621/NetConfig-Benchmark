#!/usr/bin/env python3
"""
åŸºäºLLMçš„åä¸ºNE40Eè®¾å¤‡æ‰‹å†Œæ•°æ®æ•´åˆè„šæœ¬
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½è§£æMDæ–‡æ¡£ï¼Œç”Ÿæˆç»“æ„åŒ–çš„JSONæ•°æ®
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Union
import time
import httpx
from openai import OpenAI

def extract_think_content(text):
    """
    æå–<think>å’Œ</think>æ ‡ç­¾ä¹‹é—´çš„å†…å®¹ï¼Œå¹¶è¿”å›æ ‡ç­¾å¤–çš„å†…å®¹
    :param text: åŒ…å«<think>æ ‡ç­¾çš„å­—ç¬¦ä¸²
    :return: (æ ‡ç­¾å¤–å†…å®¹, æ ‡ç­¾å†…å†…å®¹) å…ƒç»„ï¼Œè‹¥æœªæ‰¾åˆ°åˆ™æ ‡ç­¾å†…å†…å®¹ä¸ºNone
    """
    match = re.search(r"<think>(.*?)</think>", text, re.DOTALL)
    if match:
        reasoning_content = match.group(1).strip()
        # å»é™¤<think>...</think>éƒ¨åˆ†ï¼Œä¿ç•™å¤–éƒ¨å†…å®¹
        response_content = (text[:match.start()] + text[match.end():]).strip()
        return response_content, reasoning_content
    return text, None

class LLMClient:
    def __init__(self, api_key="sk-7b818fc469ff47fa8d95d7b24a530869", base_url="https://api.deepseek.com", model="deepseek-chat"):
    # def __init__(self, api_key="sk-GdmMOsWLYBdMwUBwJsaZGKOhM0k7cfuonqzTPvzLVVo1N4SL", base_url="https://chat.cloudapi.vip/v1/", model="claude-sonnet-4-20250514"):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ã€‚

        :param api_key: è®¿é—®LLMæ‰€éœ€çš„å¯†é’¥
        :param base_url: LLMçš„åŸºç¡€URL
        :param model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.base_url = base_url
        self.model = model
        
        # é…ç½®ä»£ç†æ”¯æŒ
        http_proxy = os.getenv("PES_HTTP_PROXY") or os.getenv("HTTP_PROXY")
        https_proxy = os.getenv("PES_HTTPS_PROXY") or os.getenv("HTTPS_PROXY")
        
        client_kwargs = {
            "api_key": self.api_key, 
            "base_url": self.base_url,
            "timeout": 60.0  # å¢åŠ è¶…æ—¶æ—¶é—´
        }
        
        # å¦‚æœæœ‰ä»£ç†è®¾ç½®ï¼Œæ·»åŠ ä»£ç†é…ç½®
        if http_proxy or https_proxy:
            # ä½¿ç”¨ HTTPS ä»£ç†ï¼ˆä¼˜å…ˆï¼‰æˆ– HTTP ä»£ç†
            proxy_url = https_proxy if https_proxy else http_proxy
            
            client_kwargs["http_client"] = httpx.Client(
                proxy=proxy_url,
                timeout=60.0
            )
            print(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
        
        self.client = OpenAI(**client_kwargs)

    def send_prompt(self, prompt):
        """
        å‘é€Promptåˆ°æ¨¡å‹å¹¶è·å–å“åº”ã€‚

        :param prompt: å‘é€ç»™æ¨¡å‹çš„Promptå­—ç¬¦ä¸²
        :return: è§£æåçš„æ–‡æœ¬å“åº”å’Œæ¨ç†è¿‡ç¨‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        try:
            response_content, reasoning_content = self.call_llm(prompt)
            return response_content, reasoning_content
        except Exception as e:
            raise RuntimeError(f"Error calling LLM API: {e}")

    def call_llm(self, prompt):
        """
        è°ƒç”¨æ¨¡å‹APIã€‚

        :param prompt: å‘é€ç»™æ¨¡å‹çš„Promptå­—ç¬¦ä¸²
        :return: å“åº”å†…å®¹å’Œæ¨ç†è¿‡ç¨‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        messages = [{"role": "user", "content": prompt}]
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.6,
            max_tokens=8096
        )
        # æå–å“åº”å†…å®¹å’Œæ¨ç†è¿‡ç¨‹
        response_content = response.choices[0].message.content
        if self.model == "deepseek-reasoner":
            reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
        elif "claude" in self.model:
            response_content, reasoning_content = extract_think_content(response.choices[0].message.content)
        else:
            reasoning_content = "éæ·±åº¦æ€è€ƒæ¨¡å‹ï¼Œæ— æ¨ç†ä¸Šä¸‹æ–‡"
        return response_content, reasoning_content

class HuaweiDataLLMIntegrator:
    def __init__(self, base_dir: str = "datasets/Huawei", api_key: str = None, base_url: str = None, model: str = None):
        self.base_dir = Path(base_dir)
        self.example_dir = self.base_dir / "example"
        self.images_dir = self.base_dir / "extracted_images"
        self.output_dir = self.base_dir / "llm_integrated_data"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
        if model:
            client_kwargs["model"] = model
        
        self.client = LLMClient(**client_kwargs)
        
        # è·å–å¯ç”¨å›¾ç‰‡åˆ—è¡¨
        self.available_images = self.get_available_images()
    
    def get_available_images(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨"""
        if not self.images_dir.exists():
            return []
        
        images = []
        for img_file in self.images_dir.glob("*.png"):
            images.append(img_file.name)
        return sorted(images)
    
    def construct_prompt(self, md_content: str, md_filename: str, available_images: List[str]) -> str:
        """æ„é€ LLMå¤„ç†çš„prompt"""
        
        # ä»ç°æœ‰çš„æˆåŠŸç¤ºä¾‹ä¸­é€‰æ‹©ä¸€äº›ä½œä¸ºfew-shot examples
        examples = """
ç¤ºä¾‹1 - OSPFåŸºæœ¬åŠŸèƒ½ï¼š
```json
{
  "topology": "DeviceA å’Œ DeviceB ä½œä¸ºABRï¼Œè¿æ¥åŒºåŸŸ0ã€åŒºåŸŸ1å’ŒåŒºåŸŸ2ã€‚DeviceC å’Œ DeviceE ä½äºåŒºåŸŸ1ã€‚DeviceD å’Œ DeviceF ä½äºåŒºåŸŸ2ã€‚",
  "requirement": "æ‰€æœ‰çš„è·¯ç”±å™¨éƒ½è¿è¡ŒOSPFï¼Œå¹¶å°†æ•´ä¸ªè‡ªæ²»ç³»ç»Ÿåˆ’åˆ†ä¸º3ä¸ªåŒºåŸŸã€‚å…¶ä¸­ï¼ŒDeviceAå’ŒDeviceBä½œä¸ºABRï¼ˆåŒºåŸŸè¾¹ç•Œè·¯ç”±å™¨ï¼‰æ¥è½¬å‘åŒºåŸŸä¹‹é—´çš„è·¯ç”±ã€‚é…ç½®å®Œæˆåï¼Œæ¯å°è·¯ç”±å™¨éƒ½åº”å­¦åˆ°è‡ªæ²»ç³»ç»Ÿå†…çš„æ‰€æœ‰ç½‘æ®µçš„è·¯ç”±ã€‚",
  "steps": [
    "åœ¨å„è·¯ç”±å™¨ä¸Šä½¿èƒ½OSPFã€‚",
    "æŒ‡å®šä¸åŒåŒºåŸŸå†…çš„ç½‘æ®µã€‚",
    "é…ç½®OSPFåŒºåŸŸçš„å¯†æ–‡éªŒè¯æ¨¡å¼ã€‚"
  ],
  "configs": {
    "DeviceA": "#\\nsysname DeviceA\\n#\\nrouter id 1.1.1.1\\n#\\ninterface GigabitEthernet1/0/0\\n undo shutdown\\n ip address 192.168.0.1 255.255.255.0\\n#\\nospf\\n area 0.0.0.0\\n  network 192.168.0.0 0.0.0.255\\n#\\nreturn",
    "DeviceB": "#\\nsysname DeviceB\\n#\\nrouter id 2.2.2.2\\n#\\ninterface GigabitEthernet1/0/0\\n undo shutdown\\n ip address 192.168.0.2 255.255.255.0\\n#\\nospf\\n area 0.0.0.0\\n  network 192.168.0.0 0.0.0.255\\n#\\nreturn"
  },
  "related_images": [
    "å›¾1-42 é…ç½® OSPF åŸºæœ¬åŠŸèƒ½ç»„ç½‘å›¾.png"
  ]
}
```

ç¤ºä¾‹2 - BGPè´Ÿè½½åˆ†æ‹…ï¼š
```json
{
  "topology": "ç»„ç½‘éœ€æ±‚å¦‚å›¾1154æ‰€ç¤ºï¼Œæ‰€æœ‰è·¯ç”±å™¨éƒ½é…ç½®BGPï¼ŒDeviceAå’ŒDeviceBåœ¨AS100ä¸­ï¼ŒDeviceCã€DeviceDå’ŒDeviceEåœ¨AS200ä¸­ã€‚",
  "requirement": "é€šè¿‡é…ç½®BGPéç­‰å€¼è´Ÿè½½åˆ†æ‹…å¯ä»¥æ ¹æ®æ¯æ¡è·¯ç”±çš„å¸¦å®½å€¼åŠ¨æ€åˆ†é…æµé‡ï¼Œå‡å°‘ç½‘ç»œæ‹¥å¡ï¼Œå……åˆ†åˆ©ç”¨ç½‘ç»œèµ„æºã€‚",
  "steps": [
    "åœ¨DeviceAå’ŒDeviceCã€DeviceDä¹‹é—´é…ç½®EBGPè¿æ¥ã€‚",
    "é…ç½®æ‰©å±•å›¢ä½“å±æ€§å‘å¸ƒç»™å¯¹ç­‰ä½“ã€‚",
    "é…ç½®éç­‰å€¼è´Ÿè½½åˆ†æ‹…åŠŸèƒ½ã€‚"
  ],
  "configs": {
    "DeviceA": "#\\nsysname DeviceA\\n#\\nbgp router-id 1.0.0.1\\n#\\nreturn"
  },
  "related_images": [
    "å›¾1-154 é…ç½®BGP éç­‰å€¼è´Ÿè½½åˆ†æ‹…ç»„ç½‘å›¾.png"
  ]
}
```
"""
        
        # æ„é€ å¯ç”¨å›¾ç‰‡åˆ—è¡¨å­—ç¬¦ä¸²
        images_list = "\n".join([f"- {img}" for img in available_images[:50]])  # é™åˆ¶æ•°é‡é¿å…promptè¿‡é•¿
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œé…ç½®æ–‡æ¡£è§£æä¸“å®¶ï¼Œéœ€è¦å°†åä¸ºç½‘ç»œè®¾å¤‡çš„Markdowné…ç½®æ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–çš„JSONæ•°æ®ã€‚

## ä»»åŠ¡è¦æ±‚

å°†ä»¥ä¸‹Markdownæ–‡æ¡£è§£æä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

1. **topology**: ç½‘ç»œæ‹“æ‰‘æè¿°ï¼ˆä»"ç½‘ç»œæ‹“æ‰‘"ã€"æ‹“æ‰‘ç»“æ„"æˆ–"ç»„ç½‘éœ€æ±‚"ç« èŠ‚æå–ï¼‰
2. **requirement**: ç»„ç½‘éœ€æ±‚æè¿°ï¼ˆä»"ç»„ç½‘éœ€æ±‚"ã€"é…ç½®éœ€æ±‚"ç« èŠ‚æå–ï¼‰  
3. **steps**: é…ç½®æ€è·¯æ­¥éª¤æ•°ç»„ï¼ˆä¼˜å…ˆä»"é…ç½®æ€è·¯"ç« èŠ‚æå–ï¼Œå¦‚æ²¡æœ‰åˆ™ä»æ“ä½œæ­¥éª¤ä¸­æå–ä¸»è¦æ­¥éª¤ï¼‰
4. **configs**: è®¾å¤‡é…ç½®å­—å…¸ï¼Œkeyä¸ºè®¾å¤‡åï¼Œvalueä¸ºé…ç½®å†…å®¹
5. **related_images**: ç›¸å…³å›¾ç‰‡æ•°ç»„ï¼ˆä»æ–‡æ¡£ä¸­æåˆ°çš„å›¾åºå·åŒ¹é…ï¼Œå¦‚"å›¾1-42"ç­‰ï¼‰

## é‡è¦å¤„ç†è§„åˆ™

### é…ç½®å¤„ç†è§„åˆ™ï¼š
- é…ç½®å‘½ä»¤ä¸­å¦‚é‡åˆ° `#command` æ ¼å¼ï¼Œéœ€è¦æ‹†åˆ†ä¸ºä¸¤è¡Œï¼š`#` å’Œ `command`
- ä¾‹å¦‚ï¼š`#sysname DeviceA` åº”è¯¥å¤„ç†ä¸º `#\\nsysname DeviceA`
- é…ç½®å†…å®¹ä½¿ç”¨ `\\n` è¡¨ç¤ºæ¢è¡Œ

### å›¾ç‰‡åŒ¹é…è§„åˆ™ï¼š
- ä»”ç»†æŸ¥æ‰¾æ–‡æ¡£ä¸­æåˆ°çš„å›¾åºå·ï¼Œå¦‚"å›¾1-42"ã€"å¦‚å›¾1-77æ‰€ç¤º"ç­‰
- æ ¹æ®æ–‡æ¡£å†…å®¹å’ŒåŠŸèƒ½åŒ¹é…æœ€ç›¸å…³çš„å›¾ç‰‡
- ä¼˜å…ˆåŒ¹é…åè®®ç±»å‹ï¼ˆOSPFã€BGPã€RIPç­‰ï¼‰å’ŒåŠŸèƒ½æè¿°
- å¯¹äºæ–‡ä»¶åï¼š`{md_filename}`ï¼Œé‡ç‚¹å…³æ³¨å…¶ä¸­çš„åè®®å’ŒåŠŸèƒ½å…³é”®è¯

### å†…å®¹æå–è§„åˆ™ï¼š
- topology: ç®€æ´æè¿°ç½‘ç»œæ‹“æ‰‘ç»“æ„å’Œè®¾å¤‡è§’è‰²
- requirement: å®Œæ•´æè¿°é…ç½®éœ€æ±‚å’Œç›®æ ‡
- steps: æå–é…ç½®æ€è·¯æˆ–ä¸»è¦æ“ä½œæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å­—ç¬¦ä¸²
- configs: æå–æ¯ä¸ªè®¾å¤‡çš„å®Œæ•´é…ç½®ï¼Œä¿æŒåŸæœ‰æ ¼å¼

## å‚è€ƒç¤ºä¾‹

{examples}

## å¯ç”¨å›¾ç‰‡åˆ—è¡¨
{images_list}

## å¾…å¤„ç†æ–‡æ¡£

æ–‡ä»¶å: {md_filename}

æ–‡æ¡£å†…å®¹:
```markdown
{md_content}
```

## è¾“å‡ºè¦æ±‚

è¯·ç›´æ¥è¾“å‡ºJSONæ ¼å¼çš„ç»“æœï¼Œç¡®ä¿ï¼š
1. JSONæ ¼å¼æ­£ç¡®ä¸”å®Œæ•´
2. å­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œä½¿ç”¨ `\\n` è¡¨ç¤º  
3. é…ç½®ä¸­çš„ `#command` æ ¼å¼å·²æ­£ç¡®å¤„ç†
4. å›¾ç‰‡åŒ¹é…å‡†ç¡®ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹å’ŒåŠŸèƒ½ç›¸å…³æ€§
5. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œç›´æ¥è¾“å‡ºJSON

è¾“å‡ºJSON:"""
        
        return prompt
    
    def call_llm(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """è°ƒç”¨LLM API"""
        for attempt in range(max_retries):
            try:
                # åœ¨promptå‰æ·»åŠ ç³»ç»ŸæŒ‡ä»¤
                enhanced_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œé…ç½®æ–‡æ¡£è§£æä¸“å®¶ï¼Œä¸“é—¨å¤„ç†åä¸ºç½‘ç»œè®¾å¤‡é…ç½®æ–‡æ¡£ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼æ•°æ®ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

{prompt}"""
                
                response_content, reasoning_content = self.client.send_prompt(enhanced_prompt)
                
                # å¦‚æœæœ‰æ¨ç†è¿‡ç¨‹ï¼Œæ‰“å°å‡ºæ¥ï¼ˆå¯é€‰ï¼‰
                if reasoning_content and reasoning_content != "éæ·±åº¦æ€è€ƒæ¨¡å‹ï¼Œæ— æ¨ç†ä¸Šä¸‹æ–‡":
                    print(f"ğŸ¤” æ¨ç†è¿‡ç¨‹: {reasoning_content[:200]}...")
                
                return response_content.strip()
                
            except Exception as e:
                print(f"è°ƒç”¨LLM APIå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    print(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None
        
        return None
    
    def parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”å¹¶éªŒè¯JSONæ ¼å¼"""
        if not response:
            return None
        
        try:
            # å°è¯•æå–JSONå†…å®¹ï¼ˆå¤„ç†å¯èƒ½çš„markdownæ ¼å¼ï¼‰
            json_content = response.strip()
            
            # å¦‚æœå“åº”åŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSON
            if "```json" in json_content:
                # æå–```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                start_marker = "```json"
                end_marker = "```"
                start_idx = json_content.find(start_marker)
                if start_idx != -1:
                    start_idx += len(start_marker)
                    end_idx = json_content.find(end_marker, start_idx)
                    if end_idx != -1:
                        json_content = json_content[start_idx:end_idx].strip()
            elif "```" in json_content and json_content.count("```") >= 2:
                # å¤„ç†é€šç”¨ä»£ç å—æ ¼å¼
                parts = json_content.split("```")
                if len(parts) >= 3:
                    json_content = parts[1].strip()
            
            # å°è¯•è§£æJSON
            data = json.loads(json_content)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["topology", "requirement", "steps", "configs"]
            for field in required_fields:
                if field not in data:
                    print(f"LLMå“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                    return None
            
            # éªŒè¯æ•°æ®ç±»å‹
            if not isinstance(data["steps"], list):
                print("stepså­—æ®µåº”è¯¥æ˜¯æ•°ç»„")
                return None
            
            if not isinstance(data["configs"], dict):
                print("configså­—æ®µåº”è¯¥æ˜¯å¯¹è±¡")
                return None
            
            return data
            
        except json.JSONDecodeError as e:
            print(f"è§£æLLMå“åº”JSONå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response[:300]}...")
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            try:
                # ç§»é™¤å¯èƒ½çš„é¢å¤–å­—ç¬¦
                cleaned_response = response.strip()
                if cleaned_response.startswith("```json"):
                    cleaned_response = cleaned_response[7:]
                if cleaned_response.endswith("```"):
                    cleaned_response = cleaned_response[:-3]
                
                # å†æ¬¡å°è¯•è§£æ
                data = json.loads(cleaned_response.strip())
                print("âœ… JSONä¿®å¤æˆåŠŸ")
                return data
                
            except:
                print(f"JSONä¿®å¤ä¹Ÿå¤±è´¥äº†")
                return None
        
        except Exception as e:
            print(f"è§£æå“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return None
    
    def process_md_file(self, md_file: Path) -> Optional[Dict]:
        """ä½¿ç”¨LLMå¤„ç†å•ä¸ªmdæ–‡ä»¶"""
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"æ­£åœ¨ä½¿ç”¨LLMå¤„ç†: {md_file.name}")
            
            # æ„é€ prompt
            prompt = self.construct_prompt(content, md_file.name, self.available_images)
            
            # è°ƒç”¨LLM
            llm_response = self.call_llm(prompt)
            if not llm_response:
                print(f"  -> LLMè°ƒç”¨å¤±è´¥: {md_file.name}")
                return None
            
            # è§£æå“åº”
            result = self.parse_llm_response(llm_response)
            if not result:
                print(f"  -> è§£æLLMå“åº”å¤±è´¥: {md_file.name}")
                return None
            
            print(f"  -> LLMå¤„ç†æˆåŠŸ: {md_file.name}")
            return result
            
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {md_file} æ—¶å‡ºé”™: {e}")
            return None
    
    def process_all_md_files(self, limit: int = None):
        """å¤„ç†æ‰€æœ‰mdæ–‡ä»¶"""
        if not self.example_dir.exists():
            print(f"æºç›®å½•ä¸å­˜åœ¨: {self.example_dir}")
            return
        
        md_files = list(self.example_dir.glob("*.md"))
        if limit:
            md_files = md_files[:limit]
        
        processed_count = 0
        failed_count = 0
        
        for md_file in md_files:
            result = self.process_md_file(md_file)
            if result:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                output_filename = md_file.stem + ".json"
                output_path = self.output_dir / output_filename
                
                # ä¿å­˜jsonæ–‡ä»¶
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                processed_count += 1
                print(f"  -> å·²ç”Ÿæˆ: {output_path}")
            else:
                failed_count += 1
                print(f"  -> å¤„ç†å¤±è´¥: {md_file.name}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(1)
        
        print(f"\\nå¤„ç†å®Œæˆ!")
        print(f"æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
        print(f"å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LLMå¤„ç†åä¸ºç½‘ç»œé…ç½®æ–‡æ¡£")
    parser.add_argument("--api-key", help="APIå¯†é’¥")
    parser.add_argument("--base-url", help="APIåŸºç¡€URL")
    parser.add_argument("--model", help="ä½¿ç”¨çš„LLMæ¨¡å‹")
    parser.add_argument("--provider", choices=["deepseek", "claude", "openai"], 
                       default="deepseek", help="LLMæä¾›å•†")
    parser.add_argument("--limit", type=int, help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()
    
    # æ ¹æ®æä¾›å•†è®¾ç½®é»˜è®¤å€¼
    if args.provider == "deepseek":
        default_api_key_env = "DEEPSEEK_API_KEY"
        default_base_url = "https://api.deepseek.com"
        default_model = "deepseek-chat"
        api_description = "DeepSeek APIå¯†é’¥"
    elif args.provider == "claude":
        default_api_key_env = "ANTHROPIC_API_KEY" 
        default_base_url = "https://chat.cloudapi.vip/v1/"
        default_model = "claude-sonnet-4-20250514"
        api_description = "Claude APIå¯†é’¥"
    elif args.provider == "openai":
        default_api_key_env = "OPENAI_API_KEY"
        default_base_url = "https://api.openai.com/v1"
        default_model = "gpt-4"
        api_description = "OpenAI APIå¯†é’¥"
    else:
        # é»˜è®¤ä½¿ç”¨deepseek
        default_api_key_env = "DEEPSEEK_API_KEY"
        default_base_url = "https://api.deepseek.com"
        default_model = "deepseek-chat"
        api_description = "DeepSeek APIå¯†é’¥"
    
    # è·å–é…ç½®å‚æ•°
    api_key = args.api_key or os.getenv(default_api_key_env)
    base_url = args.base_url or default_base_url
    model = args.model or default_model
    
    if not api_key:
        print(f"é”™è¯¯: è¯·æä¾›{api_description}")
        print(f"æ–¹æ³•1: è®¾ç½®ç¯å¢ƒå˜é‡ {default_api_key_env}")
        print(f"æ–¹æ³•2: ä½¿ç”¨ --api-key å‚æ•°")
        return
    
    print(f"ä½¿ç”¨{args.provider}æä¾›å•†")
    print(f"æ¨¡å‹: {model}")
    print(f"APIåœ°å€: {base_url}")
    
    integrator = HuaweiDataLLMIntegrator(
        api_key=api_key, 
        base_url=base_url,
        model=model
    )
    integrator.process_all_md_files(limit=args.limit)

if __name__ == "__main__":
    main()